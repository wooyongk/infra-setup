x-airflow-common:
  &airflow-common
  build: 
    dockerfile: Airflow.Dockerfile
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: 'Asia/Seoul'
    AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: 'Asia/Seoul'
    AIRFLOW__METRICS__STATSD_ON: 'true'
    AIRFLOW__SCHEDULER__STATSD_HOST: 'statsd-exporter'
  volumes:
    - airflow-dags-volume:/opt/airflow/dags
    - airflow-logs-volume:/opt/airflow/logs
  user: "${AIRFLOW_UID:-50000}:0"

services:
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 3G

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 4G  

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 1G

  airflow-init:
    << : *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - airflow db init &&
        airflow users create
        --role Admin
        --username ${_AIRFLOW_WWW_USER_USERNAME}
        --password ${_AIRFLOW_WWW_USER_PASSWORD}
        --email airflow@airflow.com
        --firstname ${_AIRFLOW_WWW_USER_FIRSTNAME}
        --lastname ${_AIRFLOW_WWW_USER_LASTNAME}

  statsd-exporter:
    image: prom/statsd-exporter
    container_name: airflow-statsd-exporter
    command: "--statsd.listen-udp=:8125 --web.listen-address=:9102"
    ports:
        - 9123:9102
        - 8125:8125/udp
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  git-sync:
    image: databurst/git-sync:latest
    container_name : git-sync
    environment:
      REPO_URL: https://github.com/wooyongk/data-engineering-project.git
      GIT_BRANCH: main
      DIRECTORY_NAME: project
      DESTINATION_PATH: /app/sync 
      SUBFOLDER_PATH: /airflow/dags
      INTERVAL: 10
    volumes:
      - airflow-dags-volume:/app/sync
    depends_on:
      airflow-init:
        condition: service_completed_successfully

volumes:
  airflow-dags-volume:
  airflow-logs-volume: